{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Advanced Portfolio Construction and Analysis with Python***\n",
    "\n",
    "[https://www.coursera.org/learn/advanced-portfolio-construction-python/](https://www.coursera.org/learn/advanced-portfolio-construction-python/)\n",
    "\n",
    "Compiled by *ruud waij*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Section 1\n",
    "## Video: Introduction to factor investing\n",
    "### Smart beta\n",
    "Another name for factor investing is [smart beta](https://www.investopedia.com/terms/s/smart-beta.asp). It consists of two parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- passive, beta: index investing (S&P 500)\n",
    "- active: active trading, possibly outperforming the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Active manager attempts to bring his or her skills to provide better risk adjusted returns that the benchmark index itself can deliver.\n",
    "\n",
    "Factor investing is rule-based active investing without the subjective views of a manager.\n",
    "\n",
    "*Indicization* refers to the trend of creating new indices that capture the portion of active management that is rules based and systematic, and in the long run should outperform the cap-weighted benchmark.\n",
    "\n",
    "A *factor* is a variable that influences the returns of assets. It represents a commonality in the returns  of assets, something outside of the individual asset.\n",
    "\n",
    "Types of factors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- macro factors: (industrial) growth, inflation\n",
    "- statistical factors: information extracted from the data that may not be identifiable\n",
    "- intrinsic factors or style factors: value-growth, momentum, low volatility ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Factor models and the CAPM\n",
    "### Factor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the factor model decomposes return $R$ into the sum of the premia. The premium is the return that you (can) get in exchange for exposing yourself to that factor.\n",
    "$$R_i=\\beta_1f_1+\\beta_2f_2+ \\dotsc + \\beta_3 f_3 + \\alpha + \\varepsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $R_i$: the return\n",
    "- $\\beta$: multiplier\n",
    "- $f$: return of a factor\n",
    "- $\\alpha$: fixed component\n",
    "- $\\varepsilon$: error term, the part of the return that factors cannot explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAPM\n",
    "*CAPM* stands for [capital asset pricing model](https://www.investopedia.com/terms/c/capm.asp) and it is a strange omission that this is not mentioned in the video!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E(r_i) -r_f= \\frac{ \\mathit{cov}(r_i,r_m)}{\\mathit{var}(r_m)}(E(r_m)-r_f)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E(r_i)-r_f= \\beta_i(E(r_m)-r_f)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $E(r_i)-r_f$: excess return of an asset $i$ over the risk-free rate $r_f$\n",
    "- $E(r_m)-r_f$: excess return of the market $m$ over the risk-free rate\n",
    "- $\\beta_i$: the factor for asset $i$. *[$\\beta$](https://www.investopedia.com/terms/b/beta.asp) is a measure of the volatility of a security or portfolio compared to the market as a whole.*\n",
    "- $\\mathit{cov}(r_i,r_m)$: covariance of the return of asset $i$ and the return of the market $m$.\n",
    "- $\\mathit{var}(r_m)$: the variance of the return of the market $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the excess return of an asset is based on the market return and the risk free rate, all assets (should, but don't) line up on a straight line: [the security market line](https://en.wikipedia.org/wiki/Security_market_line) (see image below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SML-chart](images/SML-chart.png)\n",
    "\n",
    "[Image from Wikipedia](https://en.wikipedia.org/w/index.php?curid=40048738)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** According to the CAPM, the $\\alpha$ term in the CAPM Factor Model is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zero\n",
    "- One\n",
    "- Zero if Epsilon is Zero, One otherwise\n",
    "- Depends on the Risk Free Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Zero:* Correct. The CAPM predicts that the alpha term is zero\n",
    "\n",
    "- *One*: This should not be selected. A Factor Model decomposes the return to Factor Returns, Alpha and an Error Term (epsilon) and the CAPM predicts that the excess return of a stock is a multiple of that stock’s Beta relative to the market \n",
    "\n",
    "- *Zero if Epsilon is Zero, One otherwise.*: This should not be selected. Epsilon is the Error Term \n",
    "\n",
    "- *Depends on the Risk Free Rate.*: This should not be selected. The CAPM only uses the risk free rate to compute excess returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CAPM with just one factor is not accurate. That is why there are multi-factor models that change CAPM anomalies into regular factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Multi-Factor models and Fama-French\n",
    "### Fama-French\n",
    "The [Fama-French](https://www.investopedia.com/terms/f/famaandfrenchthreefactormodel.asp) model is an extension of the CAPM model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size\n",
    "In the Fama-French model the stocks are sorted according to market capitalization. Small-cap stocks on average outperform large-cap stocks. this is called the *size-effect* and is not explained by the CAPM-model ([CAPM](#CAPM))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value vs. growth\n",
    "The Fame-French model shows that [value stocks](https://www.investopedia.com/terms/v/valuestock.asp) outperform [growth stocks](https://www.investopedia.com/search?q=growth+stocks). This is probably why [book to price ratio](https://www.investopedia.com/terms/b/booktomarketratio.asp) is briefly mentioned in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: A Manager tells you that he concentrates his portfolio in Value stocks because Value outperforms Growth, and his portfolio has outperformed the S&P500 for the last 3 years. Assuming his statements are all True, which of the following statements can you conclude from this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The portfolio will outperform the S&P500 next year\n",
    "-The portfolio will outperform the S&P500 next year if the [Value factor](https://www.risk.net/definition/value-factor) has a positive risk premium next year\n",
    "- If the manager does not show style drift AND the Value Factor generates a positive risk premium the next year, THEN the manager is likely to outperform, but it is not a certainty\n",
    "- None of the Above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: If the manager does not show style drift ([style drift](#Style-drift)) AND the Value Factor generates a positive risk premium the next year, THEN the manager is likely to outperform, but it is not a certainty.\n",
    "Correct \n",
    "\n",
    "\"The Factors Mimicking Portfolios are broad portfolio and it is possible to see a return that is different from the Factor Mimicking portfolio.\" (This is not a very clear explanation. Factor mimicking is mentioned (not even explained) after this question. Sloppy work.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fama and French (1993)\n",
    "The model includes a *small* and *value* factor to the *market* factor.\n",
    "$$E[r_i] = r_f +\\beta_{i,\\mathit{MKT}}E[r_m-r_f]+\\beta_{i,\\mathit{SMB}} E[\\mathit{SMB}]+\\beta_{i,\\mathit{HML}} E[\\mathit{HML}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mathit{MKT}$: market factor\n",
    "- $\\mathit{SMB}$: small minus big stocks\n",
    "- $\\mathit{HML}$: high book/price (value) minus low book/price (growth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fama and French interpret the small stock effect and the value effect as being systematic factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMB and HML are zero cost portfolios so the factors $\\beta_{i,\\mathit{SMB}}$ and $\\beta_{i,\\mathit{HML}}$ are centered around zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to *value* the following factors are recognized:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- low 'vol' (=volatility) beats high vol\n",
    "- high quality beats low quality (??)\n",
    "- [momentum](https://www.investopedia.com/articles/technical/081501.asp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although *size* is relevant, it is not seen as a factor. The factors mentioned above are applied to small caps portfolios and large caps portfolios. (Probably, large caps portfolio are desirable, despite having a lower return.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factors can be used as diagnostic tools to decompose returns. This can be used to perform [style analysis](https://www.investopedia.com/terms/s/style_analysis.asp) to determine investment behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Factor benchmarks and style analysis\n",
    "### Style analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a portfolio with $\\beta=1.3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E(r_i - r_f)=\\alpha+1.3 E(r_m-r_f)$$\n",
    "$$E(r_i)=\\alpha+[-0.3r_f+1.3E(r_m)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor benchmark $[-0.3r_f+1.3E(r_m)]$ is a short position of $\\$0.30$ in cash (T-bills) and a leveraged position of $\\$1.30$ in the market portfolio. This can be earned without intervention by an asset manager. The $\\alpha$ can be seen as the value that was added by the manager.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Assume the risk free rate is 1% per year and the Stock Market returned 11% in a given year. An Active Manager “beat the market” and generated a 14% return and had a Beta of 1.3. Did the manager generate an $\\alpha=0$; $\\alpha>0$; $\\alpha<0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** $r_f=1\\%$, $r_m=11\\%$, $r_i= 14\\%$, $\\beta=1.3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E(r_i - r_f)=\\alpha+\\beta E(r_m-r_f)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha = E(r_i - r_f) - \\beta E(r_m-r_f) = 14-1 - 1.3*(11-1)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharpe style analysis\n",
    "Model:\n",
    "$$R_m = W_1R_{i1}+W_2R_{i2}+W_3R_{i3}+\\alpha+\\varepsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $W_i$: weight of a part $i$ of the portfolio with $\\sum_i{W_i}=1$ and all $W_i>0$.\n",
    "- $R_i$: return of a particular type of investment (oil companies, european companies, it could be anything). It is an explanatory variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You run a [regression](https://www.investopedia.com/terms/r/regression.asp) to determine if $\\alpha>0$. If it is, that is due to the actions of the manager. The regression is solved through [quadratic programming](https://en.wikipedia.org/wiki/Quadratic_programming), repeated for a sliding window of 1-3 years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality of fit:\n",
    "$$\\mathit{PSEUDO}\\, R^2 = \\frac{\\mathit{VAR}(R_m) - \\mathit{VAR}(\\varepsilon)}{\\mathit{VAR}(R_m)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style drift\n",
    "As the time window moves, you can see the weights $W_i$ change. This is called *style drift*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Section 2\n",
    "## video: Shortcomings of cap-weighted indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inefficiency of cap-weighted (cw) benchmarks\n",
    "Portfolios that use *cw* indices are well within the [efficient frontier](https://www.investopedia.com/terms/e/efficientfrontier.asp). they do not give the highest return for a given level of volatility. However, the portfolios on the efficient frontier suffer from [look-ahead bias](https://www.investopedia.com/terms/l/lookaheadbias.asp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Platen and Rendek (2010)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2170212) found that the Sharpe-ratio of equally weighted (ew) portfolios was higher than that of the cap-weighted portfolio. \n",
    "The cap-weighted portfolio is not well diversified and holds unrewarded risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart weighted benchmarks\n",
    "- equally-weighted benchmarks\n",
    "- minimum variance benchmarks\n",
    "- risk parity benchmarks\n",
    "\n",
    "### Monkeys!\n",
    "[Clare, Motson and Thomas (2013)](https://www.cass.city.ac.uk/faculties-and-research/research/cass-knowledge/2013/april/monkeys-vs-fund-managers-an-evaluation-of-alternative-equity-indices) found that randomly selecting and weighting stocks outperforms the cw index. [This story explains their approach.](https://www.cass.city.ac.uk/faculties-and-research/research/cass-knowledge/inbusiness/2013/monkey-business)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: From cap-weighted benchmarks to smart-weighted benchmarks\n",
    "### shortcomings of cap-weighted indices\n",
    "- Cw indices have an inefficient diversification because high allocation to large cap stocks and [growth stocks](https://www.investopedia.com/articles/professionals/072415/value-or-growth-stocks-which-best.asp). There are unrewarded [specific risks](https://www.investopedia.com/terms/s/specificrisk.asp) that lead to a sub-optimal risk reward ratio.\n",
    "- Cw indices provide an inefficient exposure to rewarded [systematic risks](https://www.investopedia.com/video/play/systematic-risk/). As explained by Fama and French ([Fama and French](#Fama-and-French-(1993))), small caps stocks and value stocks tend to have a higher reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart factor benchmarks\n",
    "A smart factor benchmark can be constructed using this two step process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select the type of factor exposure that you want to hold in your portfolio (value, size, momentum, volatility (see [Other factors](#Other-factors)))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|       Valuation      |              Size             |               Momentum               |       Volatility      |   |\n",
    "|:--------------------:|:-----------------------------:|:------------------------------------:|:---------------------:|---|\n",
    "|         Value        |           Large Cap           |             Past Winners             |        High Vol       |   |\n",
    "|        Growth        |            Mid Cap            |              Past Losers             |        Low Vol        |   |\n",
    "| Book to market ratio | Freefloat adjusted market cap | Cumulative return over the past year | Vol of weekly returns |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select your preferred weighting scheme(s).\n",
    "- Equally-Weighted\n",
    "- Efficient Minimum Variance\n",
    "- Risk Parity\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Section 1\n",
    "## Video: The curse of dimensionality\n",
    "### Problem\n",
    "We have to estimate the parameters for all $N$ portfolio constituents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $N$ return parameters \n",
    "- $N$ volatility parameters\n",
    "- $N*(N-1)/2$ correlation parameters\n",
    "\n",
    "This gives too much data. We might have to deal with 10 years of daily returns for 5000 stocks: $10*250*5000$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the number of parameters required for mean-variance optimization based on the S&P 500 universe, which contains 500 stocks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** $N=500: N+N+ N*(N-1)/2 \\rightarrow 125750$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible cures\n",
    "- increase the sample size to estimate all parameters accurately\n",
    "  - increase sample period\n",
    "  - increase frequency \n",
    "- decrease number of parameters\n",
    "  - decrease number of assets $N$.\n",
    "  - decrease the number of parameters for a fixed $N$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme example 1: no model risk - high sample risk\n",
    "Reduce the correlation parameters to a sample covariance estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{S}_{ij} = \\frac{1}{t} \\sum_{t=1}^{T}(R_{ijt}-\\bar{R}_i)(R_{jt}-\\bar{R}_{j})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\bar{R}_i = \\frac{1}{T}\\sum_{t=1}^T R_{it}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\bar{R}_j = \\frac{1}{T}\\sum_{t=1}^T R_{jt}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\hat{S}_{ij}$: sample covariance estimate\n",
    "- $R_{it}$: historical return of $R_i$ at time $t$.\n",
    "- $T$: the observed period\n",
    "- $\\bar{R}_i$: the mean of $R_i$ over time $T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme example 2: low sample risk - high model risk\n",
    "This approach uses a constant correlation model (CC) in which all $N(N-2)/2$ individual correlation parameters $\\rho_{ij}$ tween returns are replaced with a single correlation parameter $\\hat{\\rho}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{\\sigma}_{ij}^{CC} = \\hat{\\sigma}_i \\hat{\\sigma}_j\\hat{\\rho}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\hat{\\sigma}_{ij}^{CC}$: constant correlation model covariance estimate for $i,j$\n",
    "- $\\hat{\\sigma}_i$: estimator for stock $i$ volatility\n",
    "- $\\hat{\\rho}$: correlation parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This single correlation parameter is estimated by the average:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\rho} = \\frac{1}{N(N-1)}\\sum_{i,j=1; i\\neq j}^N  \\hat{\\rho}_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the number of parameter estimates required for mean-variance optimization based on the S&P 500 universe, when using the constant correlation covariance matrix estimate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We need 500 expected return estimates ($\\bar{R}_i$), 500 volatility parameter estimates ($\\hat{\\sigma}_i$), and also one correlation parameter estimate ($\\hat{\\rho}$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Estimating the Covariance Matrix with a Factor Model\n",
    "### Factor-based covariance estimate\n",
    "Assume stock returns are driven by a limited set of factors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R_{it} = \\mu_i + \\beta_{i1}F_{1t} + \\dotsi + \\beta_{ik}F_{kt} + \\dotsi + \\beta_{iK}F_{Kt} + \\epsilon_{it}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $R_{it}$: return of asset $i$ at time $t$\n",
    "- $\\mu_i$: ??\n",
    "- $\\beta_{ik}$: sensitivity of asset $i$ with respect to factor $F_{kt}$\n",
    "- $F_{kt}$: factor \n",
    "- $\\epsilon_{it}$: error term, the part that is not explained by the factor model.\n",
    "- $K$: the number of factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance for the 2-factor case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_l^2 = \n",
    "\\beta_{il}^2\\sigma_{F1}^2 +\n",
    "\\beta_{i2}^2\\sigma_{F2}^2 +\n",
    "2\\beta_{il}\\beta_{i2}\\mathit{Cov}(F_l,F_2)+\\sigma_{\\epsilon l}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance for the 2-factor case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_{ij}=\n",
    "\\beta_{il}\\beta_{jl} \\sigma_{F1}^2 +\n",
    "\\beta_{i2}\\beta_{j2} \\sigma_{F2}^2 +\n",
    "(\\beta_{il}\\beta_{j2}+ \\beta_{i2}\\beta_{jl})\\mathit{Cov}(F_1,F_2)+\n",
    "\\mathit{Cov}(\\epsilon_{it},\\epsilon_{jt})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the error terms $\\epsilon_{it}$ and $\\epsilon_{jt}$ are uncorrelated. This means that the [specific risk](https://www.investopedia.com/terms/s/specificrisk.asp) for these stocks are uncorrelated. We introduce *model risk* to reduce the number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathit{Cov}(\\epsilon_{it},\\epsilon_{jt})=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case with uncorrelated parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General composition of returns for $K$ factors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathit{cov}(R_i(t),R_j(t)) =\n",
    "\\sum_{k=1}^K \\beta_{ik}\\beta_{jk}\\sigma_{F_k}^2+\n",
    "\\mathit{cov}(\\epsilon_i(t),\\epsilon_j(t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the error terms are uncorrelated $\\mathit{cov}(\\epsilon_{it},\\epsilon_{jt})=0$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_{ij} = \\mathit{cov}(R_i(t),R_j(t)) = \\sum_{k=1}^K \\beta_{ik}\\beta_{jk}\\sigma_{F_k}^2  \\quad  \\mathit{for}\\; i\\neq j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_{ii} = \\sigma_i^2= \\mathit{cov}(R_i(t),R_i(t)) = \n",
    "\\sum_{k=1}^K \\beta_{ik}^2 \\sigma_{F_k}^2  \\quad  \\mathit{for}\\; i = j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How many parameters do you need to estimate when using a 2-factor models for estimating the covariance matrix of a universe of 500 stocks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "We first need 500 volatility estimates $\\sigma_i$ for individual stock returns ($i=1\\dotsi500$), plus 500 estimates of betas of stocks with respect to factor $k=1$ ($\\beta_{i1}$), 500 estimates of betas of stocks with respect to factor $k=2$ ($\\beta_{i2}$), and finally 2 volatility estimates for factor returns, which gives a total of 500+500+500+2=1,502, which compares favorably to 500x499/2=124,750 when using the sample covariance matrix estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of factor model\n",
    "The simplest model is Sharpe's single-factor market model  (1963):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R_{i,t}-r_{f,t} =\n",
    "\\alpha_i +\n",
    "\\beta_i(R_{M,t}-r_{f,t})+\\epsilon_{i,t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three families of factor models:\n",
    "- explicit macro factor model with inflation, growth, interwst rates, time spread (??)\n",
    "- explicit micro factor model with stock specific factors: country, industry, size, book-to-market. \n",
    "- implicit model with statistical factors: perform statistical analysis on data to determine orthogonal uncorrelated factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Honey I Shrunk the Covariance Matrix!\n",
    "### Shrinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curse of dimensionality can be handled by a new methodology: [shrinkage](http://www.ledoit.net/honey.pdf).\n",
    "\n",
    "There is a trade-off between sample risk and model risk. Sample-based estimates for covariance parameters have lot of sample risk,\n",
    "too many parameters to estimate, but there is no model risk. \n",
    "\n",
    "There are other methodologies like the constant correlation methodology \n",
    "(see [constant correlation](###Extreme-example-2-low-sample-risk---high-model-risk))\n",
    "or factor based methodology (see [factor based covariance estimate](###Factor-based-covariance-estimate)) that suffer from a lower degree of sample risk, because they allow to reduce the number of parameters to estimate. But that came at the cost of\n",
    "introducing some kind of structure, and therefore there is some fair amount of model risk.\n",
    "\n",
    "Statistical shrinkage mixes the two methodologies to deliver the optimal trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{S}_{\\mathit{shrink}} = \\hat{\\delta}^*\\hat{F}+(1-\\hat{\\delta}^*)\\hat{S}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\hat{S}_{\\mathit{shrink}}$: covariance metrics parameter of the covariance matrix\n",
    "- $\\hat{\\delta}^*$: percentage to mix the two estimators described below\n",
    "- $\\hat{F}$: the factor model-based estimator for the covariance matrix (with model risk)\n",
    "- $\\hat{S}$:  data based estimate for the covariance matrix (with sample risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Consider two stocks with sample volatility estimates at 20\\% and 30\\%, respectively, and sample correlation at .75. Further assume that the average of the sample correlation estimates of all stocks in the universe is .5. What is for these two stocks the sample-based covariance estimate, the constant correlation covariance estimate and the covariance estimate based on statistical shrinkage with a shrinkage factor of 50\\%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The sample-based estimate \n",
    "$\\hat{\\sigma}_{1,2} = \\sigma_1 \\sigma_2\\rho_{1,2}$ \n",
    "is \n",
    "$20\\%*30\\%*0.75=0.045$. \n",
    "The constant correlation estimate \n",
    "$\\hat{\\sigma}_{ij}^{CC} = \\hat{\\sigma}_i \\hat{\\sigma}_j\\hat{\\rho}$\n",
    "is $20\\%*30\\%*0.5=0.03$. \n",
    "The shrinkage estimate \n",
    "$\\hat{\\sigma}_{1,2}^{shrink} = \\delta \\hat{\\sigma}_{1,2}+(1-\\delta) \\hat{\\sigma}_{ij}^{CC}$\n",
    "with \n",
    "$\\delta=0.5$ is \n",
    "$(0.045+0.03)/2=0.0375$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing statistical shrinkage is formally equivalent to introducing min/max weight constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just LeDoit in Python: [sklearn.covariance.LedoitWolf](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Section 2\n",
    "## Video: Portfolio Construction with Time-Varying Risk Parameters\n",
    "### Estimating volatility\n",
    "There is a curse of non-stationarity: the parameters vary over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $\\sigma_T$ as the volatility between day $T$ and day $T+1$ as estimated at the end of day $T$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_T^2 = \\frac{1}{T} \\sum_{t=1}^T(R_t-\\bar{R})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\bar{R} = \\frac{1}{T}\\sum_{t=1}^{T}R_{t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the mean $\\bar{R}=0$ ($R_t$ is centered around zero). The variance $\\sigma_T^2$ simplifies to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_T^2 = \\frac{1}{T} \\sum_{t=1}^T R_t^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Consider the following stream of returns: +1\\%, -2\\%, -1\\%, +2\\%. What are the corresponding (arithmetic) average return and volatility estimates? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.015811388300841896)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "returns= pd.Series([0.01,-0.02,-0.01,0.02])\n",
    "returns.mean(), returns.std(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Average return is (1%-2%-1%+2%)/4=0%. Variance of returns is (1%2-2%2-1%2+2%2)/4=0.025%. Volatility is square-root of variance: √0.025%=1.58%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of non-stationarity\n",
    "When trying to reduce sample risk (see [sample risk](###Extreme-example-1-no-model-risk---high-sample-risk)), it is better to increase the frequency than increasing the time period in case of non-stationary return distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding window analysis\n",
    "As time goes by, you add data to calculate the estimate of the volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling window analysis\n",
    "As new data becomes available, you remove the oldest data to calculate the exstimate of volatility. The size of the observation window remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What type of data would give you the best estimation power for covariance matrix parameters, assuming constant parameters? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Weekly data for 5 year. This gives you 52x5=260 data points. Of course, if risk parameters are time-varying, it may not be such a good idea to use data extending over such a long time period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Exponentially weighted average\n",
    "### Historical volatility estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_T^2 = \\frac{1}{T}\\sum_{t=1}^T R_t^2 \\quad \\mathit{if}\\; \\bar{R}=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above formula each data point contributes with weight $\\alpha=\\frac{1}{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_T^2 = \\sum_{t=1}^T \\alpha_t R_t^2 \\quad \\mathit{where}\\; \\sum_{t=1}^T \\alpha_t=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EWMA model\n",
    "In an *exponentially weighted moving average* model ([EWMA](https://www.investopedia.com/articles/07/ewma.asp)) the weights decline exponentially as we move back through time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha_t = \\frac{\\lambda^{T-t}}{\\sum_{t=1}^T\\lambda^{T-1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\lambda^T$: decay factor ($0 < \\lambda < 1$), a low factor puts emphasis on recent data points. $\\lambda=0.9$ has been found to be a good value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance parameter estimate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathit{cov}(R_i,R_j) = \\sum_{t=1}^T \\alpha_t (R_{i,t}-\\bar{R_i})(R_{j,t}-\\bar{R_j})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because EWMA puts emphasis on more recent data, we can use *expanding window analysis* (see [expanding window analysis](###Expanding-window-analysis)) and do not have to rely on *rolling window analysis*. (On the other hand, why not limit the data that you use for calculations if part of the data contributes very little??)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with rolling window analysis\n",
    "is that as long as\n",
    "the data point is within the rolling window,\n",
    "it matters, and whenever it is out of the rolling window,\n",
    "it doesn't matter at all.\n",
    "So the day before it gets out,\n",
    "it's as important as the most recent observation,\n",
    "the next day it's out of\n",
    "the rolling window and no longer matters. \n",
    "It is more intuitive to let\n",
    "the importance of each observation\n",
    "decrease gradually over time. We keep all observations\n",
    "and we use a weighting scheme\n",
    "that gives more importance\n",
    "to recent observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: ARCH and GARCH Models\n",
    "### ARCH model\n",
    "[ARCH](https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity) stands for *autoregressive conditional heteroskedasticity*. 'In statistics, a vector of random variables is [heteroscedastic](https://en.wikipedia.org/wiki/Heteroscedasticity) (or heteroskedastic from Ancient Greek hetero “different” and skedasis “dispersion”) if the variability of the random disturbance is different across elements of the vector.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an *ARCH(T) model* we assign some weight to the long-run variance $V_L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma^2_T = \\gamma V_L + \\sum_{t=1}^T \\alpha_t R_t^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "$$\\gamma + \\sum_{t=1}^T\\alpha_t=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ARCH(1) model*\n",
    "$$\\sigma^2_T = \\gamma V_L + \\alpha R_T^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "$$\\gamma + \\alpha=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GARCH model\n",
    "[GARCH](https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity#GARCH) stands for *generalized autoregressive conditional heteroskedasticity model*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GARCH(1,1) we additionally assign some weight to the previous variance estimate to capture *volatility clustering*. Levels of volatility values are clustered in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_T^2 = \\gamma V_L + \\alpha R_T^2 + \\beta \\sigma_{T-1}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with\n",
    "$$\\gamma + \\alpha + \\beta = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\sigma_T^2$: new estimate of volatility\n",
    "- $\\gamma V_L$: contribution of long term volatility\n",
    "- $\\alpha R_T^2$: contribution of last return\n",
    "- $\\beta \\sigma_{T-1}^2$: contribution of previous estimate of volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Suppose the estimation of a GARCH(1,1) model on daily data gives:\n",
    "$$\\sigma^2_T = 0.000002+0.13R^2_T + 0.86\\sigma_{T-1}^2$$\n",
    "and also suppose the last daily estimate of the volatility is 1.6% per day and the most recent percentage change in the market variable is 1%. (The change in the market variable is the return! I wonder if *variable* is a typo and *value* was meant.) What is the new daily volatility estimate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** $\\gamma V_L = 0.000002 \\quad \\alpha=0.13 \\quad \\beta=0.86$\n",
    "\n",
    "Mind that the GARCH formula uses variance, not volatility (=standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015334927453366058"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "alpha=0.13\n",
    "beta=0.86\n",
    "gammaV=0.000002\n",
    "return_val= 0.01\n",
    "volatility=0.016\n",
    "math.sqrt(gammaV + alpha * return_val*return_val + beta * volatility*volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations on GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In model [GARCH(P,Q)](https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity#GARCH(p,_q)_model_specification) $p$ is the number of past return data points and $q$ is the number of contributing previous volatility estimates to compute the volatility estimate. $\\omega$ is the contribution of the long term volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_T^2= \\omega +\n",
    "\\sum_{i=1}^p\\alpha_i R_{T-i}^2 + \n",
    "\\sum_{j=1}^q\\beta_j \\sigma_{T-j}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\omega = \\gamma V_L$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take into account that volatility changes over time, we introduce additional parameters: $\\alpha_1 \\dotsc \\alpha_p\n",
    "\\quad \\beta_1 \\dotsc \\beta_q \\quad \\gamma$. This increases the curse of dimensionality (see [The curse of dimensionality](##Video--The-curse-of-dimensionality))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Garch\n",
    "The *orthogonal (O)GARCH model* is a factor model for explaining co-variance terms\n",
    "between two different assets, and it is a factor model with uncorrelated orthogonal factors. It only allows for time variation in the variance of the factors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\sigma}_{ij}^{OGARCH} =\n",
    "\\hat{\\sigma}_{ij}(t) = \\sum_{k=1}^K \\hat{\\beta}_{ik}  \\hat{\\beta}_{jk} \\hat{\\sigma}_{F_k}^2(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How many parameters do you need to estimate when using a 2-factor models with GARCH(1,1) model for the volatility of each one of the two factors? **Not mentioned:** you have a 500 stock portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We first need 500 volatility estimates for individual stock returns, plus 500 estimates of betas of stocks with respect to factor 1, 500 estimates of betas of stocks with respect to factor 2, and finally 3 GARCH parameter estimates for each factor, which gives a total of 500+500+500+2x3=1,506, which is not much more than if we had assumed constant volatility parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Section 1\n",
    "## Video: Lack of Robustness of Expected Return Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sample based information is, unfortunately, close to useless when it comes to expected return estimation. \n",
    "Sample based expected return\n",
    "can be very sample dependent.\n",
    "Small changes in the sample will lead to large\n",
    "changes in the sample based estimate for expected returns.\n",
    "Sample based expected return estimators are extremely noisy, especially for high volatility portfolios.\n",
    "So, the confidence intervals are very large, we have very little\n",
    "confidence that the estimator that we come up with is of any meaningfulness.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentist versus Bayesian statics.\n",
    "In [Frequentist statistics](https://en.wikipedia.org/wiki/Frequentist_inference) information is only gathered from taking samples. [Bayesian statistics](https://en.wikipedia.org/wiki/Bayesian_inference) uses prior knowledge to draw conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian statistics and statistical shrinkage again!\n",
    "The sample mean estimate might be improved by shrinking the individual means to the *grand sample mean*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\bar{\\mu_i}= \\frac{1}{T}\\sum_{t=0}^{T-1} R_{t,t+1}^i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^N \\bar{\\mu_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\mu_i} = \\delta \\bar{\\mu} + (1-\\delta) \\bar{\\mu_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $N$: number of stocks\n",
    "- $T$: the number of (equally spaced in time) samples for each stock $i$.\n",
    "- $\\bar{\\mu_i}$: sample based average return for each stock $i$ between $t$ and $t+1$ (this is noisy data).\n",
    "- $\\bar{\\mu}$: the average of all $\\bar{\\mu_i}$ ([grand mean](https://en.wikipedia.org/wiki/Grand_mean))\n",
    "- $\\hat{\\mu_i}$: expected return of each stock $i$  of the $N$ stocks, using shrinkage\n",
    "- $\\delta$: shrinkage factor (0..1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Consider 3 assets with sample means equal to 10%, 15% and 20%, and assume a shrinkage factor d=50%. What is the shrinkage estimator for the expected return on these 3 assets? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0  0.125\n",
       "1  0.150\n",
       "2  0.175"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sample_means= pd.DataFrame([0.1,0.15,0.2])\n",
    "grand_mean= sample_means.mean()\n",
    "delta= 0.5\n",
    "shrinkage_means= delta*grand_mean + (1-delta)* sample_means\n",
    "shrinkage_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Grand mean = (10%+15%+20%)/3 = 15%. Shrinkage estimator for asset 1: 50% x 10% + 50% x 15% = 12.5%. Shrinkage estimator for asset 2: 50% x 15% + 50% x 15% = 15%. Shrinkage estimator for asset 1: 50% x 20% + 50% x 15% = 17.5%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Agnostic Priors on Expected Return Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximize the Sharpe Ratio of a portfolio $\\mathit{SR}_p$, we need we need the expected return of the portfolio $\\mu_p$, for which we need the expected returns of the individual components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathit{SR}_p \\equiv \\frac{\\mu_p-r}{\\sigma_p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First agnostic prior: expected returns are all equal\n",
    "We can use as prior knowledge that the expected return of each component is equal to the grand mean (see [Grand mean](###Bayesian-statistics-and-statistical-shrinkage-again!)). The estimate for all returns is now equal, independent of their volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second agnostic prior: Sharpe ratios are all equal\n",
    "Sharpe ratios are constant across assets. Excess expected return (return minus risk free return) is proportional to volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu_i - r_f = \\lambda \\sigma_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\lambda$: Sharpe ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathit{SR}_p =\n",
    "\\frac{\\sum_{i=1}^N w_i(\\mu_i - r_f)}\n",
    "{ \\sqrt{ \\sum_{i,j=1}^N w_i w_j \\sigma_{ij} } }\n",
    "=\n",
    "\\frac{\\sum_{i=1}^N w_i(\\mu - r_f)}\n",
    "{ \\sqrt{ \\sum_{i,j=1}^N w_i w_j \\sigma_{ij} } }\n",
    "=\n",
    "\\lambda\n",
    "\\frac{\\sum_{i=1}^N w_i\\sigma_i}\n",
    "{ \\sqrt{ \\sum_{i,j=1}^N w_i w_j \\sigma_{ij} } }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\sum_{i=1}^N w_i\\sigma_i$: weighted average of the component volatilities\n",
    "- $\\sqrt{ \\sum_{i,j=1}^N w_i w_j \\sigma_{ij} }$: portfolio volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can maximize the portfolio Sharpe ratio by maximizing the numerator given by\n",
    "the weighted average of the volatilities divided by portfolio volatility. This ratio is known as *diversification ratio*. We do not have to know what the value of $\\lambda$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the Sharpe ratio of a portfolio of an equally-weighted portfolio of two stocks with volatility respectively equal to 20% and 30%, and a pairwise correlation .6, assuming that they both have a 70% Sharpe ratio? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $w_1= 0.5$ (weight)\n",
    "- $w_2= 0.5$\n",
    "- $\\sigma_1= 0.2$ (volatility)\n",
    "- $\\sigma_2= 0.3$\n",
    "- $\\rho=0.6$ (correlation)\n",
    "- $\\lambda=0.7$ (Sharpe ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\rho(X,Y) = \\frac { \\mathit{cov}(X,Y)}{\\sqrt{\\mathit{var}(X) \\mathit{var} (Y)}}\n",
    "\\Rightarrow\n",
    "\\mathit{cov}(X,Y) = \\rho(X,Y) \\sqrt{\\mathit{var}(X) \\mathit{var} (Y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathit{var}(X) = \\sigma_X^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathit{SR}_p = \\lambda \\frac{\\sum_{i=1}^N w_i \\sigma_i}\n",
    "{\\sqrt{ \\sum_{i=1}^N \\sum_{j=1}^N w_i w_j \\sigma_{ij} }}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7787397791074733"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "w1= 0.5\n",
    "w2= 0.5\n",
    "sigma1= 0.2\n",
    "sigma2= 0.3\n",
    "rho= 0.6\n",
    "lambdaa=0.7\n",
    "\n",
    "var1= sigma1*sigma1\n",
    "var2= sigma2*sigma2\n",
    "cov12= rho*sigma1*sigma2\n",
    "\n",
    "# see SRp formula above\n",
    "SRp= lambdaa* (w1*sigma1 + w2*sigma2)/ math.sqrt( w1*w1*var1 +2*w1*w2*cov12 +w2*w2*var2  )\n",
    "SRp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** 77.87%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewarded versus unrewarded risk\n",
    "Asset pricing theory suggests that only [systematic risk](https://www.investopedia.com/video/play/systematic-risk/) is rewarded. [Specific risk](https://www.investopedia.com/terms/s/specificrisk.asp) can be diversified away. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Well, in this context,\n",
    "we may want to assume that\n",
    "all stocks have the same Sharpe ratio.\n",
    "We may want to assume that there's a relationship\n",
    "between excess expected return and not total risk,\n",
    "but the systematic part of volatility.\n",
    "So in other words, we may want decompose\n",
    "volatility in terms of specific risk and systematic risk,\n",
    "and relate and come\n",
    "up with a better estimate for expected returns by\n",
    "relating it to systematic risk\n",
    "as opposed to relating it to total risk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Assume that a stock index and a bond index have the same Sharpe ratio, and that this common Sharpe ratio is equal to 50%. Further assume that interest rate is 2% and that stock index volatility is 20% and bond index volatility is 10%. Calculate the stock index expected return and the bond index expected return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu = \\lambda \\sigma+ r_f$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mu$: expected return\n",
    "- $\\lambda = 0.5$\n",
    "- $r_f=0.02$\n",
    "- $\\sigma_{bi}= 0.1$\n",
    "- $\\sigma_{si}= 0.2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** bond: 0.5\\*0.1+0.02= 7%;  stock: 0.5\\*0.2+0.02= 12%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Using Factor Models to Estimate Expected Returns\n",
    "### CAPM-based expected return estimates\n",
    "*If CAPM (see [CAPM](###CAPM)) is the true asset pricing model, then the excess expected return is proportional to $\\beta$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu_i -r_f = \\beta_i (\\mu_M - r_f)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mu_i$: expected return of stock $i$\n",
    "- $r_f$: riskfree rate\n",
    "- $\\beta_i$: factor for stock $i$\n",
    "- $\\mu_M$: market return\n",
    "- $\\mu_M - r_f$: market risk premium for systematic risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Treynor ratio](https://www.investopedia.com/terms/t/treynorratio.asp):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathit{TR}_i = \\frac{\\mu_i-r_f}{\\beta_i} = \\mu_M-r_f$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Treynor ratio has an identical value for all stocks: $\\mu_M - r_f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Assume that a stock has an expected return of 13% and a volatility of 20%. Further assume that the risk-free rate is 3%, and the stock beta is 1. What is the Sharpe ratio (SR) and Treynor ratio (TR) for this stock?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mu_i= 0.13$\n",
    "- $\\sigma_i= 0.2$\n",
    "- $r_f= 0.03$\n",
    "- $\\beta_i= 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathit{SR}_i= \\frac{\\mu_i-r_f}{\\sigma_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathit{TR}_i= \\frac{\\mu_i-r_f}{\\beta_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** SR= (0.13-0.03)/0.2= 50% TR= (0.13-0.03)/1= 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAPM is not the true asset pricing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Assume that last year volatility is 20% and last year performance is 15% for stock 1, while it is 30% and 18% respectively for stock 2. Further assume that stock 1 beta is 1.1 and stock 2 beta is 0.8. Which stock should have the highest expected return if the CAPM is correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu_i -r_f = \\beta_i (\\mu_M - r_f) \\Rightarrow \\mu_i = \\beta_i (\\mu_M - r_f) +r_f$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** according to CAPM (see [CAPM](###CAPM)) excess return is fully predicted by the value of $\\beta$. Stock 1 has the higher value of $\\beta$. Historic return values have no predictive power. CAPM does not consider volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected return estimates with mult-factor models\n",
    "With multi-factor models excess expected returns are given by a combination of risk exposures times factor premia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stephen Ross Arbitrage pricing theory](https://www.investopedia.com/terms/a/apt.asp):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu_i-r_f = \n",
    "\\sum_{k=1}^K \\beta_{ik} (\\mu_k-r_f) =\n",
    "\\sum_{k=1}^K \\beta_{ik} \\lambda_k \\sigma_k \n",
    "\\quad \\mathrm{with} \\, \\lambda_k = \\frac{\\mu_k-r_f}{\\sigma_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mu_i-r_f$: excess expected return for stock $i$\n",
    "- $\\beta_{ik}(\\mu_k-r_f)$: the contribution of the excess expected return of stock $k$ to that of $i$\n",
    "- $\\beta_{ik}$: the factor of stock $k$ for stock $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be able to estimate the excess returns $u_k-r_f$ of the components, which is equally difficult as estimating the expected return of $\\mu_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different approaches:\n",
    "1. *agnostic approach*: assume that all factors have the same Sharpe ratio\n",
    "2. *frequentist approach*: determine the Sharpe ratios by looking at the longest possible sample.\n",
    "3. *Active approach*: the use of qualitative or quantitative analysis by a portfolio manager (back to voodoo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Assume that a stock has a beta of 1 with respect to factor 1 and a beta of .5 with respect to factor 2. Further assume that the excess expected return is 6% on factor 1 and 8% on factor 2, and that the risk-free rate is 2%.  What is the expected return on the stock, assuming that these two factors are the only rewarded factors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\beta_1= 1$\n",
    "- $\\beta_2= 0.5$\n",
    "- $\\mu_1-r_f= 0.06$\n",
    "- $\\mu_2-r_f= 0.08$\n",
    "- $r_f=0.02$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** the expected return $\\beta_1 (\\mu_1-r_f) + \\beta_2 (\\mu_2-r_f) + r_f = 1*0.06+0.5*0.08+0.02= 12\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Section 2\n",
    "## Video: Extracting Implied Expected Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [Black-Litterman model](https://www.investopedia.com/terms/b/black-litterman_model.asp) (BL) you have active views of expected returns and those are used in portfolio construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding an anchor point\n",
    "BL uses a preferred benchmark as as a starting point (anchor) in the portfolio construction process. If the portfolio manager does not have a lot of confidence in his views, the portfolio should mainly consist of assets from the benchmark.\n",
    "\n",
    "BL uses Bayesian analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neutral/implied expected returns\n",
    "The neutral prior distribution is obtained by reverse engineering, assuming the benchmark is the optimal portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engineering of portfolio construction:\n",
    "- start with expected returns $\\mu_i$ and  and covariances $\\sigma_{i,j}$ \n",
    "- determine the weights $w_i^*$ of the maximum Sharpe ratio portfolio (denoted by $*$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(\\mu_i,\\sigma_{i,j})_{i,j=1\\dotsc N} \\longrightarrow (w_i^*)_{i,j=1\\dotsc N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there are no meaningful estimates of expected returns. We cannot determine stable Sharpe ratio maximizing weights $w_i^*$.\n",
    "\n",
    "Instead we do reverse engineering (reverse portfolio optimization) and start with weights for a selected benchmark (cap-weighted, equally weighted, any benchmark).\n",
    "\n",
    "- take as input:\n",
    "  - the covariance parameters $\\sigma_{i,j}$ \n",
    "  - the weights of the preferred benchmark $w_i^{benchmark}$.\n",
    "- Calculate (*extract*) the (implied) expected returns (vector $\\Pi$) that you would get from using these weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(\\sigma_{i,j}, w_i^{benchmark})_{i,j=1\\dotsc N} \\longrightarrow \\Pi = (\\mu_i^{implied})_{i=1\\dotsc N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Assume two uncorrelated stocks with volatility 10% and 15% respectively. Further assume that the risk-free rate is 0%. What can we say about the neutral expected returns consistent with an equally-weighted benchmark portfolio?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The official answer claims that we know that the Sharpe ratio maximizing weights are proportional to $(\\mu_i -r)/\\sigma_i^2$. I have no [recollection](http://waij.com/documents/coursera/edhec/investment_python/) of that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mu_1/\\sigma_1^2= \\mu_2/\\sigma_2^2= 50\\% \\quad \\mu_1/\\mu_2 = \\sigma_2^2/\\sigma_1^2= 2.25$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bayesian prior* is that true expected returns are centered at market implied values denoted by $\\Pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu = \\Pi + \\varepsilon^e$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\varepsilon^e \\sim N(0,\\tau \\Sigma)$ (0 ??)\n",
    "- $\\tau$ is a scalar indicating the uncertainty of the prior\n",
    "- $\\Sigma = (\\sigma_{ij})_{i,j=1\\dotsc N}$ is the covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Introducing Active Views\n",
    "The Black-Litterman approach mixes market (benchmark) implied expected returns with the manager's active views about those expected returns.\n",
    "\n",
    "### Active view\n",
    "An *active view* is expressed as a statement that the expected return on a given asset within\n",
    "a portfolio $P$ has a normal distribution with mean $Q$ and standard deviation $\\Omega$. $Q$ is a vector of mean values of $K$ views in the portfolio. The view may concern an asset or a set of assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P\\mu = Q + \\varepsilon^v \\quad \\mathrm{where} \\, \\varepsilon^v \\sim N(0,\\Omega)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $N$ is the total number of assets\n",
    "- $N(0,\\Omega)$ is the normal distribution??\n",
    "- $K$ is the total number of views that will be expressed.\n",
    "- $P$ is a $K \\times N$ matrix that identifies the assets involved in the views.\n",
    "- $Q$ is a $K$-vector of expected returns on these portfolios or assets (??).\n",
    "- $\\Omega$ is a $K \\times N$ matrix of error terms in the views (confidence levels)\n",
    "- $\\varepsilon^v$: uncertainty in the views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Black-Litterman model\n",
    "The Black-Litterman model combines benchmark implied expected returns $\\Pi$ (prior return vector):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu = \\Pi + \\varepsilon^e \\quad \\mathrm{where} \\, \\varepsilon^e \\sim N(0,\\tau \\Sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with active views:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P\\mu = Q + \\varepsilon^v \\quad \\mathrm{where} \\, \\varepsilon^v \\sim N(0,\\Omega)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Bayesian framework the new expected returns $\\bar{\\mu}$ (posterior return vector) can be written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\bar{\\mu}=\n",
    "\\left[(\\tau \\Sigma)^{-1} + \n",
    "P^\\prime \\Omega^{-1} P \\right]^{-1}\n",
    "\\left[(\\tau \\Sigma)^{-1} \\Pi +\n",
    "P^\\prime \\Omega^{-1} Q \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\bar{\\mu}$: expected returns vector\n",
    "- $\\Pi$: the bench-marked implied expected return\n",
    "- $Q$: the active views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Black-Litterman Analysis\n",
    "### Expected returns - no views\n",
    "The historical mean is a noisy estimate for expected return. The expected return based on the CAPM-model is less noisy. \n",
    "\n",
    "(I probably somehow missed it, but I do not remember seeing the CAPM expected return clearly explained. I found the explanation on [Investopedia](https://www.investopedia.com/terms/c/capm.asp).)\n",
    "\n",
    "We start by extracting the expected return from the benchmark portfolio (cw, ew or any other benchmark). If we compare the CAPM expected returns with the benchmark expected returns, the correlation is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BL approach can be extended to any benchmark. For this just derive in the first step implied expected returns consistent with the EW portfolio being the Max Sharpe Ratio portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing active views\n",
    "THe BL model allows active views to be expressed in absolute or relative terms.\n",
    "\n",
    "We are going to\n",
    "assume that the confidence levels\n",
    "for the views are going to be proportional\n",
    "to the variance of the prior.\n",
    "Just as in one of the classical implementation of\n",
    "the Black-Litterman model that is\n",
    "displayed in a paper by [He and Litterman](https://faculty.fuqua.duke.edu/~charvey/Teaching/IntesaBci_2001/GS_The_intuition_behind.pdf) (version with good layout!) in 1999."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Section 1\n",
    "## Naive Diversification\n",
    "### Proverbial definition for diversification\n",
    "Diversification is having a well-balanced allocation of your dollars to different assets or securities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean goal versus end goal\n",
    "- *Mean goal*: well-balanced portfolio\n",
    "- *End goal*: well rewarded portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diversification diversifies away some of the unrewarded risk within\n",
    "the portfolio and that allows us to achieve the highest possible reward per unit of risk.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective number of constituents (ENC)\n",
    "What is the number of *meaningful* allocations to assets, small fractions do not help with diversification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathit{ENC} \\equiv \\left( \\sum_{i=1}^N w_i^2\\right)^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mathit{ENC}$: Effective number of constituents\n",
    "- $w_i$: the weight of asset $i$ in the portfolio\n",
    "- $N$: the number of assets in the portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**, extreme case 1: fully concentrated portfolio with $w_1=1$ and no allocation to the other $N-1$ assets. This gives $\\mathit{ENC}= \\frac{1}{1^2+0^2+ \\dotsc+0^2}=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**, extreme case 2: equally weighted portfolio with $w_i=\\frac{1}{N}$. This gives\n",
    "$\\mathit{ENC}=\n",
    "\\frac{1}{\\left(\\frac{1}{N}\\right)^2+\n",
    "\\dotsc+\\left(\\frac{1}{N}\\right)^2}=N$. \n",
    "\n",
    "The equally weighted portfolio gives the maximum value for $\\mathit{ENC}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\mathit{ENC}$ for the *S&P500* (with 500 stocks) is typically about 4 times smaller than the nominal number of constituents. In this course, ENC is expressed as a percentage: $\\mathit{ENC}_{\\mathit{S\\&P500}}=26.9\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the effective number of constituents (ENC) for a portfolio invested 20% in a 20% volatility asset and 80% in a 10% volatility asset, assuming that these assets are uncorrelated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4705882352941173"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1= 0.2\n",
    "w2= 0.8\n",
    "ENC= 1/(w1*w1+w2*w2)\n",
    "ENC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** $\\mathit{ENC}=1.47$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Scientific diversification\n",
    "### GMV\n",
    "In order to maximize the return, give a certain risk budget, we would like to hold the *Maximum sharpe Ratio* (MSR) portfolio. It is difficult to reliably get the expected return values needed to calculate the portfolio weights. Instead we can get the *Global Minimum Variance* (GMV) portfolio, this portfolio does not require estimated return estimates. (See image below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GMV-MSR](images/gmv-msr.png)\n",
    "\n",
    "[Image from ResearchGate](https://www.researchgate.net/figure/Efficient-frontier-obtained-from-four-assets-The-Global-Minimum-Variance-GMV-is-the_fig10_308883600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivation of the Global Minimum Variance portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathit{Min\\sigma_p^2} =\n",
    "\\mathit{Min} \\sum_{i=1}^N\\sum_{j=1}^N w_i w_j \\sigma_{ij} =\n",
    "\\mathit{Min} \\sum_{i=1}^N\\sum_{j=1}^N w_i w_j \\sigma_i \\sigma_j \\rho_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with GMV portfolios\n",
    "GMV assumes equal expected returns for all assets in the portfolio, which is unlikely. When expected returns are equal, the MSR and GMV portfolio coincide. This means we can use the quadatic optimizer to find the MSR portfolio (as we did in [the first course](http://waij.com/documents/coursera/edhec/investment_python/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer will not optimize on high returns (since they are all equal), but will over-weight low volatility components. the result of this is that GMV is not a well balanced portfolio. It has been found that GMV is not consistently better than the equally weighted portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving GMV portfolios\n",
    "There are several ways to improve the performance of GMV:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum value for ENC\n",
    "GMV with an imposed minimum value for ENC (see [ENC](###Effective-number-of-constituents-(ENC))) mixes scientific diversification with naive portfolio diversification (see [Naive Diversification](##Naive-Diversification)).\n",
    "\n",
    "#### Same volatility\n",
    "Impose that all assets have the same volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathit{Min} \\sum_{i=1}^N\\sum_{j=1}^N w_i w_j \\sigma_i \\sigma_j \\rho_{ij}\n",
    "\\longrightarrow\n",
    "\\mathit{Min\\sigma^2} \\sum_{i=1}^N\\sum_{j=1}^N w_i w_j \\rho_{ij} \\quad \\mathrm{if} \\, \\sigma_i=\\sigma \\, \\mathrm{for}\\, i= 1, \\dotsc,N\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer now searches for maximally decorrelated assets in the portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Measuring risk contributions\n",
    "### Shortcomings of ENC\n",
    "Well balanced portfolios in terms of dollar contributions can be highly concentrated in terms of risk contributions. It is useful to know the contribution of each asset to the risk of the portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** 50% allocation in stock 1 with 30% volatility; 50% allocation in 10% volatility bond 2, correlation=0. What is the variance of the portfolio? (Watch out, we are mixing variance and volatility!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_p^2= \\sum_{i=1}^N\\sum_{j=1}^N w_i w_j \\sigma_i \\sigma_j \\rho_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w1=0.5$, $w2=0.5$, $\\sigma_1= 0.3$, $\\sigma_2=0.1$ and $\\rho_{ij} =0$ for $i\\neq j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma_p^2= 0.5^2 \\times 0.3^2 + 0.5^2 \\times 0.1^2 = 0.025$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of stock 1 *in the portfolio* is $50\\%^2$ of $30\\%^2$.\n",
    "The risk contribution for stock 1 is $p_1= \\frac{0.5^2 \\times 0.3^2}{\\sigma_p^2}=90\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assume, for this example, that the correlation $\\rho_{1,2} = 0.25$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma_p^2= 0.5^2 \\times 0.3^2 + 0.5^2 \\times 0.1^2 + 2 \\times 0.5 \\times 0.5 \\times 0.3 \\times 0.1 \\times 0.25= 0.0288$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocating the correlated component\n",
    "The correlated component of the variance is split by the fraction of contribution to the portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Consider a portfolio invested at 50% in a 30% volatility stock and 50% in a 10% volatility bond. What are the risk contributions in case the correlation between the stock and the bond returns is 0.5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8076923076923076"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1= 0.5; w1_2= w1*w1\n",
    "w2= 0.5; w2_2= w2*w2\n",
    "vol1= 0.3; vol1_2= vol1*vol1 #var1= vol1_2\n",
    "vol2=0.1; vol2_2= vol2*vol2\n",
    "rho= 0.5\n",
    "var_p= w1_2*vol1_2 + 2*w1*w2*vol1*vol2*rho + w2_2*vol2_2\n",
    "p1= (w1_2* vol1_2 + w1_2  * vol1 * vol2 * rho) / var_p\n",
    "p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** $p1 \\simeq 81\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Section 2\n",
    "## Video: Simplified risk parity portfolios\n",
    "### Risk parity portfolio\n",
    "*Risk parity portfolio* is a portfolio with equal risk contribution from both assets. It maximizes the ENC (see [Shortcomings of ENC](###Shortcomings-of-ENC)) applied to risk contributions, as opposed to dollar contributions. This version of ENC is called *ENCB*, known as the *effective number of correlated bets*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Assume an annualized volatility of 15.1% on a broad equity index and annualized volatility of 4.6% on a broad bond index, with a 0.2 correlation. What is the ENCB for the 60/40 portfolio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1147824352230271"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1= 0.6; w1_2= w1*w1 # 1 = equity\n",
    "w2= 0.4; w2_2= w2*w2 # 2 = bond\n",
    "vol1= 0.151; vol1_2= vol1*vol1 #var1= vol1_2\n",
    "vol2=0.046; vol2_2= vol2*vol2\n",
    "rho= 0.2\n",
    "var_p= w1_2*vol1_2 + 2*w1*w2*vol1*vol2*rho + w2_2*vol2_2\n",
    "p1= (w1_2* vol1_2 + w1_2  * vol1 * vol2 * rho) / var_p # risk contribution 1\n",
    "p2= (w2_2* vol2_2 + w2_2  * vol1 * vol2 * rho) / var_p # risk contribution 2\n",
    "# ENCB\n",
    "1/(p1*p1+p2*p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** $\\mathit{ENCB}=1.15$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two asset case, risk parity weights are proportional to the inverse of the volatilities. The risk contributions of asset $1$ and asset $2$ are equal if their weights $w_{1,2}$ satisfy the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{w_1}{w_2} = \\frac{\\sigma_2}{\\sigma_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting properties\n",
    "The risk parity portfolio is also known as the *equal risk contribution portfolio* (ERC). It is an inverse volatility weighted portfolio if all pairwise correlations are equal (important for portfolios with more than two assets). In general, pairwise correlations will not be equal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Assume that a portfolio is invested in a 20% volatility asset and in a 10% volatility asset, and further assume that these assets are uncorrelated. What is the risk parity allocation for this portfolio?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** $w_1=0.2 \\quad w_2=0.1 \\quad \\frac{w_1}{w_2} = \\frac{\\sigma_2}{\\sigma_1} = \\frac{0.1}{0.2} \\longrightarrow w_2=2*w_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Risk Parity Portfolios\n",
    "### General expressions\n",
    "The portfolio variance $\\sigma_p^2$ is the sum of the asset $i$ variances and covariances of $i,j$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_p^2= \n",
    "\\sum_{i=1}^N\\sum_{j=1}^N w_i w_j \\sigma_{ij} = \n",
    "\\sum_{i=1}^N w_i \\sigma_i^2 + \n",
    "\\sum_{i = 1}^N\\sum_{i \\neq j}^N w_i w_j \\sigma_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contribution $p_i$ of asset $i$ to the risk of the portfolio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_i=\n",
    "\\frac{w_i^2\\sigma_i^2 + \\sum_{j \\neq i}^N w_i w_j \\sigma_{ij}}\n",
    "{\\sigma_p^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effective number of correlated bets $\\mathit{ENCB}$ (similar to ENC):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathit{ENCB} = \\left( \\sum_{i=1}^N p_i^2 \\right)^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk parity portfolio\n",
    "*Risk parity portfolio*: choose portfolio (of $N$ assets) with  weights $w_i$ so as to equalize risk contributions $p_i = \\frac{1}{N}$, or equivalently, maximize $\\mathit{ENCB}$. This is also know as *equal risk contribution* (ERC) portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no analytical way to achieve risk parity, you have to use a numerical approach. Assets having the same volatility levels is not sufficient to create a risk parity portfolio because you have to take the pairwise correlation factors into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Risk parity is a naïve diversification strategy. No attempt is made to maximize the Sharpe ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Comparing Diversification Options\n",
    "### Competing portfolio construction schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For US large cap stocks (1987-2018) we compare 4 schemes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cap-weighted (cw) portfolio\n",
    "- equally-weighted (ew) portfolio \n",
    "- equal risk contribution (erc) portfolio (see [Risk parity portfolio](###Risk-parity-portfolio))\n",
    "- global minimum variance portfolio (gmv) (see [GMV](###GMV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the gmv portfolio, the minimum weight that has been chosen is $1/(3 \\times 500)$ (one third of the ew weight) and the maximum weight is $3/500$ (three times the ew weight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to cw, ew has a higher volatility but a better ENCB value (see [ENCB](###General-expressions)). ENC is much better by design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to cw, erc has a better performance, lower [max drawdown](https://www.investopedia.com/terms/m/maximum-drawdown-mdd.asp), better [Sharpe ratio](https://www.investopedia.com/terms/s/sharperatio.asp) and better ENC (see [ENC](###Effective-number-of-constituents-(ENC))). ENCB is much better by design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to cw, gmv has better performance,  lower volatility, half of the max drawdown, more than double the Sharpe ratio, better ENC and ENCB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For US large cap stocks (1987-2018), gmv has performed the best. This is because of the [bear markets](https://www.investopedia.com/terms/b/bearmarket.asp) after the tech bubble (2003) and the subprime crisis (2008)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Lab-session Risk Contributions and Risk Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word 'risk' derives from the early Italian [risicare](https://en.wiktionary.org/wiki/risicare), which means 'to dare'. In this sense, risk is a choice rather than a fate. The actions we dare to take, which depend on how free we are to make choices, are what the story of risk is all about.\n",
    "\n",
    "_Peter L. Bernstein, Against the Gods: The Remarkable Story of Risk_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
